{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "sys.path.append('/home/om2382/vanilla-rtrl/')\n",
    "from cluster import *\n",
    "from continual_learning import *\n",
    "from core import *\n",
    "from dynamics import *\n",
    "from functions import *\n",
    "from gen_data import *\n",
    "from learning_algorithms import *\n",
    "from optimizers import *\n",
    "from plotting import *\n",
    "from torch_tools import *\n",
    "from wrappers import *\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- SET UP ALL CONFIGS --- ###\n",
    "from itertools import product\n",
    "n_seeds = 50\n",
    "macro_configs = config_generator()\n",
    "micro_configs = tuple(product(macro_configs, list(range(n_seeds))))\n",
    "prototype = False\n",
    "\n",
    "### --- SELECT PARTICULAR CONFIG --- ###\n",
    "try:\n",
    "    i_job = int(os.environ['SLURM_ARRAY_TASK_ID']) - 1\n",
    "except KeyError:\n",
    "    i_job = 0\n",
    "    prototype = True\n",
    "params, i_seed = micro_configs[i_job]\n",
    "i_config = i_job//n_seeds\n",
    "\n",
    "new_random_seed_per_condition = True\n",
    "if new_random_seed_per_condition:\n",
    "    np.random.seed(i_job)\n",
    "else: #Match random seeds across conditions\n",
    "    np.random.seed(i_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_name = 'BPTT_CDM_b200_50_examples_ctx0'\n",
    "file_names = []\n",
    "\n",
    "np.random.seed(i_seed)\n",
    "T_trial = 100\n",
    "task = Context_Dependent_Decision_Task(T_trial=T_trial, input_var=0.1, report_cue=True)\n",
    "N_train = 10000000\n",
    "N_test = 20000\n",
    "checkpoint_interval = 5\n",
    "data = task.gen_data(N_train, N_test, add_dummy_end_point=False)\n",
    "batched_data = add_batch_dimension_to_data(data, T_trial)\n",
    "\n",
    "### --- initialize RNN paramters and RNN object --- ###\n",
    "n_in = task.n_in\n",
    "n_hidden = 32\n",
    "n_out = task.n_out\n",
    "\n",
    "W_in  = np.random.normal(0, np.sqrt(1/(n_in)), (n_hidden, n_in))\n",
    "W_rec = np.random.normal(0, np.sqrt(1/n_hidden), (n_hidden, n_hidden))\n",
    "W_out = np.random.normal(0, np.sqrt(1/(n_hidden)), (n_out, n_hidden))\n",
    "b_rec = np.zeros(n_hidden)\n",
    "b_out = np.zeros(n_out)\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "rnn = Torch_RNN(W_in, W_rec, W_out, b_rec, b_out,\n",
    "                activation='tanh',\n",
    "                alpha=alpha)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
    "scheduler = None\n",
    "checkpoints = train_torch_RNN(rnn, optimizer, batched_data,\n",
    "                              batch_size=200, n_epochs=2,\n",
    "                              L2_reg=0.0001, verbose=True,\n",
    "                              checkpoint_interval=checkpoint_interval,\n",
    "                              scheduler=scheduler)\n",
    "\n",
    "sim = Empty_Simulation()\n",
    "sim.checkpoints = checkpoints\n",
    "indices = sorted([k for k in checkpoints.keys() if type(k) != str])\n",
    "sim.checkpoint_interval = indices[1] - indices[0]\n",
    "sim.total_time_steps = indices[-1] + sim.checkpoint_interval\n",
    "sim.rnn = checkpoints['final']['rnn']\n",
    "\n",
    "#fix context to context 0\n",
    "task.fixed_context = 0\n",
    "\n",
    "file_name = root_name + '_seed={}'.format(i_seed)\n",
    "for key in params.keys():\n",
    "    file_name += '_{}={}'.format(key, str(params[key]).replace('.', ','))\n",
    "\n",
    "file_names.append(file_name)\n",
    "\n",
    "print('finish simulation: {}'.format(i_seed))\n",
    "for key in params.keys():\n",
    "    print('{}:, {}, seed = {}'.format(key, params[key], i_seed))\n",
    "if np.isnan(rnn.W_rec.detach().numpy()[0, 0]):\n",
    "    print('MISTAKES WERE MADE')\n",
    "    pass\n",
    "\n",
    "\n",
    "### --- SAVE RUN --- ###\n",
    "with open(os.path.join('/home/om2382/learning-dynamics/notebooks/Figure_CDM/saved_runs/', file_name), 'wb') as f:\n",
    "    saved_run = {'sim': sim, 'task': task}\n",
    "    pickle.dump(saved_run, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook generate_saved_runs_BPTT_CDM.ipynb to script\n",
      "[NbConvertApp] Writing 11582 bytes to generate_saved_runs_BPTT_CDM.py\n",
      "awk: cmd. line:1: /###Truncate/ <IPython.core.autocall.ZMQExitAutocall object at 0x2b21e1cf9970> <built-in function print>\n",
      "awk: cmd. line:1:                       ^ syntax error\n",
      "awk: cmd. line:1: /###Truncate/ <IPython.core.autocall.ZMQExitAutocall object at 0x2b21e1cf9970> <built-in function print>\n",
      "awk: cmd. line:1:                                                                                ^ syntax error\n"
     ]
    }
   ],
   "source": [
    "###Truncate file above\n",
    "file_name = 'generate_saved_runs_BPTT_CDM'\n",
    "job_name = 'generate_BPTT_CDM_b200_runs'\n",
    "project_dir = '/home/om2382/learning-dynamics/'\n",
    "main_script_path = os.path.join(project_dir, 'cluster_main_scripts', job_name + '.py')\n",
    "get_ipython().run_cell_magic('javascript', '', 'IPython.notebook.save_notebook()')\n",
    "get_ipython().system('jupyter nbconvert --to script --no-prompt {}.ipynb'.format(file_name))\n",
    "get_ipython().system('awk \"/###Truncate/ {{exit}} {{print}}\" {}.py'.format(file_name))\n",
    "get_ipython().system('sed -i \"/###Truncate/Q\" {}.py'.format(file_name))\n",
    "get_ipython().system('mv {}.py {}'.format(file_name, main_script_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘/home/om2382/learning-dynamics/results/misc/generate_BPTT_CDM_b200_runs/result_*’: No such file or directory\n",
      "sending incremental file list\n",
      "\n",
      "sent 3,494 bytes  received 34 bytes  2,352.00 bytes/sec\n",
      "total size is 441,230  speedup is 125.07\n"
     ]
    }
   ],
   "source": [
    "###Submit job to cluster\n",
    "n_jobs = len(micro_configs)\n",
    "write_job_file(job_name, py_file_name='{}.py'.format(job_name))\n",
    "job_script_path = os.path.join(project_dir, 'job_scripts', job_name + '.s')\n",
    "job_id_1 = submit_job(job_script_path, n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n",
      "           1122084    lkumar  jupyter   om2382  R      25:10      1 ax16\r\n"
     ]
    }
   ],
   "source": [
    "###Get job status\n",
    "get_ipython().system('squeue -u om2382')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -t ../job_scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/home/om2382/learning-dynamics/'\n",
    "job_name = 'generate_RFLO_CDM_runs'\n",
    "job_script_path = os.path.join(project_dir, 'job_scripts', job_name + '.s')\n",
    "configs_array, results_array, key_order, sim_dict = unpack_processed_data(job_script_path, username='om2382')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Plot examples with one task --- ###\n",
    "task_dict = {}\n",
    "for key in sim_dict.keys():\n",
    "    #T_context = int(key.split('_')[-1])\n",
    "    task_dict[key] = Context_Dependent_Decision_Task(T_trial=100, input_var=0.1, report_cue=True, tonic_report=True)\n",
    "#data = task.gen_data(1000, 1000) #generate data from task\n",
    "fig = plot_1d_or_2d_array_of_config_examples(configs_array, results_array,\n",
    "                                             key_order, sim_dict, data=data,\n",
    "                                             task_dict=task_dict, N_task_data=3000,\n",
    "                                             xlim=[0, 3000], trace_spacing=3,\n",
    "                                             output_scale=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_2d_array_of_config_results(configs_array, results_array, key_order)#, tick_rounding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_3d_or_4d_array_of_config_results(configs_array, results_array, key_order,\n",
    "                                            tick_rounding=5, vmin=0, vmax=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- COLORS --- ###\n",
    "\n",
    "def hex_to_rgb(value):\n",
    "    value = value.lstrip('#')\n",
    "    lv = len(value)\n",
    "    return list(int(value[i:i + lv // 3], 16) for i in range(0, lv, lv // 3))\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    return '#%02x%02x%02x' % rgb\n",
    "\n",
    "def linearly_interpolate_colors(col1, col2, N):\n",
    "\n",
    "    if '#' in col1:\n",
    "        c1 = hex_to_rgb(col1)\n",
    "    if '#' in col2:\n",
    "        c2 = hex_to_rgb(col2)\n",
    "\n",
    "    cols = np.linspace(c1, c2, N).astype(int)\n",
    "    return [rgb_to_hex(tuple(c)) for c in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['probe']['X'][:,0])\n",
    "plt.plot(data['probe']['X'][:,1])\n",
    "plt.plot(data['probe']['X'][:,2])\n",
    "plt.plot(data['probe']['X'][:,3])\n",
    "plt.plot(data['probe']['X'][:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- PLOT FINAL TOPOLOGIES IN SPACE --- ###\n",
    "colors = ['#0000FF', '#D62728']\n",
    "col1 = '#FF0000'\n",
    "col2 = '#0000FF'\n",
    "col3 = '#00FF00'\n",
    "col4 = '#FFFF00'\n",
    "probe_colors = linearly_interpolate_colors(col1, col2, 10) + linearly_interpolate_colors(col3, col4, 10)\n",
    "data = task.gen_data(0, 1000)\n",
    "#probe_colors = ['C{}'.format(i) for i in range(10)] + ['k']\n",
    "for i_key, key in enumerate(sim_dict):\n",
    "\n",
    "    #if key.split('_')[-1] != '0':\n",
    "    #    continue\n",
    "    if key != '0.001_0.03_0':\n",
    "        continue\n",
    "    #if '500000' not in key:\n",
    "    #    continue\n",
    "    sim = sim_dict[key]\n",
    "    cp = sim.analyzed_checkpoint_1\n",
    "    ssa = State_Space_Analysis(cp, data, n_PCs=2)\n",
    "    ssa = plot_checkpoint_results(cp, data, ssa,\n",
    "                                  plot_fixed_points=True,\n",
    "                                  plot_cluster_means=True,\n",
    "                                  plot_graph_structure=False,\n",
    "                                  plot_test_points=False,\n",
    "                                  n_test_samples=None,\n",
    "                                  T_per_sample=100,\n",
    "                                  test_alpha=1,\n",
    "                                  plot_probe=True,\n",
    "                                  n_probes=20,\n",
    "                                  probe_colors=probe_colors,\n",
    "                                  eig_norm_color=True,\n",
    "                                  graph_key='adjmat_input_1')\n",
    "    W_in_ = cp['rnn'].W_in[:,0]\n",
    "    W_out_ = cp['rnn'].W_out[0]\n",
    "    w_in = ssa.transform(W_in_)\n",
    "    w_in = w_in / norm(w_in)\n",
    "    w_out = ssa.transform(W_out_)\n",
    "    w_out = w_out / norm(w_out)\n",
    "    #plt.title(key + ', in-out-alignment = {}'.format(normalized_dot_product(W_in_, W_out_)))\n",
    "    plt.title(key)\n",
    "    if True:\n",
    "        #ssa.ax.plot([-w_in[0], w_in[0]],\n",
    "        #            [-w_in[1], w_in[1]], color='k', linewidth=2)\n",
    "        ssa.ax.plot([-w_out[0], w_out[0]],\n",
    "                    [-w_out[1], w_out[1]], color='g', linewidth=2)\n",
    "        window_size = 1.2\n",
    "        #ssa.ax.set_ylim([-window_size, window_size])\n",
    "        #ssa.ax.set_xlim([-window_size, window_size])\n",
    "    #ssa.fig.savefig('figs/DB_input_{}.pdf'.format(i_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('FPs.csv', sim_dict['3_0.1_0'].analyzed_checkpoint['fixed_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sim_dict['3_0.1_0'].analyzed_checkpoint['fixed_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize lists to store the results\n",
    "num_clusters = []\n",
    "num_noise_points = []\n",
    "\n",
    "# Iterate over the seeds\n",
    "for seed in range(10):\n",
    "    # Run t-SNE with the current seed\n",
    "    tsne = TSNE(n_components=2, random_state=seed)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    \n",
    "    # Run DBSCAN with eps=4.6 and min_samples=5\n",
    "    dbscan = DBSCAN(eps=4.6, min_samples=5)\n",
    "    db_labels = dbscan.fit_predict(tsne_data)\n",
    "    \n",
    "    # Calculate the number of clusters and the number of noise points\n",
    "    clusters = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "    noise_points = list(db_labels).count(-1)\n",
    "    \n",
    "    # Store the results\n",
    "    num_clusters.append(clusters)\n",
    "    num_noise_points.append(noise_points)\n",
    "\n",
    "# Display the results\n",
    "print(num_clusters)\n",
    "print(num_noise_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize lists to store the results\n",
    "num_clusters = []\n",
    "num_noise_points = []\n",
    "\n",
    "# Create a figure with 7 subplots\n",
    "for method in ['PCA1', 'PCA2', 'TSNE']:\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "    keys = [k for k in list(sim_dict.keys()) if k.split('_')[0]=='128']\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "\n",
    "        data = sim_dict[keys[i]].analyzed_checkpoint['fixed_points']\n",
    "        # Run t-SNE with the current seed\n",
    "        if method == 'TSNE':\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=30.0)\n",
    "            tsne_data = tsne.fit_transform(data)\n",
    "        #random projection instead\n",
    "        #if i< 9:\n",
    "        #    np.random.seed(seed)\n",
    "        #    U = np.linalg.qr(np.random.normal(0, 1, (data.shape[1], data.shape[1])))[0][:,:2]\n",
    "        #elif i==9:\n",
    "        elif method == 'PCA1':\n",
    "            U = np.linalg.svd(data)[-1][:,:2]\n",
    "            tsne_data = data.dot(U)\n",
    "        elif method == 'PCA2':\n",
    "            U = np.linalg.svd(data)[-1][:2]\n",
    "            tsne_data = data.dot(U.T)\n",
    "\n",
    "        # Run DBSCAN with eps=4.6 and min_samples=5\n",
    "        dbscan = DBSCAN(eps=5, min_samples=5)\n",
    "        db_labels = dbscan.fit_predict(tsne_data)\n",
    "\n",
    "        # Calculate the number of clusters and the number of noise points\n",
    "        clusters = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "        noise_points = list(db_labels).count(-1)\n",
    "\n",
    "        # Store the results\n",
    "        num_clusters.append(clusters)\n",
    "        num_noise_points.append(noise_points)\n",
    "\n",
    "        # Plot the t-SNE visualization for this seed\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        scatter = ax.scatter(tsne_data[:, 0], tsne_data[:, 1], c=db_labels, cmap='Spectral', s=5)\n",
    "        ax.set_title(f'Seed {0}, Clusters {clusters}, Noise {noise_points}, {keys[i]}')\n",
    "        legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "        ax.add_artist(legend1)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    #plt.suptitle(f'perp = {perplexity}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcurve.principle_curve import PrincipleCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-v-rtrl]",
   "language": "python",
   "name": "conda-env-.conda-v-rtrl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
